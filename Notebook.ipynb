{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h7wmUz0M4Vbn"
   },
   "source": [
    "# **Ajanın Öğrenme Süreci**\n",
    "---\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1000/1*aKYFRoEmmKkybqJOvLt2JQ.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*   Ajan çevreden S(t) durum değerini alır.\n",
    "*   Bu S(t) değerine göre Ajan bir A(t) eylemi belirler.\n",
    "*   Bu A(t) eylemi sonucunda bir sonraki durumumuz olan S(t+1) elde edilir.\n",
    "*   Çevre bu alınan A(t) eyleminin ödülü olan R(t)'yi ajana geri döner.\n",
    "\n",
    "\n",
    "Bu döngü ajanın eğitim setini deneme yanılma yoluyla oluşturmasını sağlar.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kZldBxMQ7SNZ"
   },
   "source": [
    "# **Değer Fonksiyonu:**\n",
    "\n",
    "---\n",
    "\n",
    "Değer fonksiyonu, ajanın bir durumda alacağı maksimum beklenen toplam getiriyi hesaplayan fonksiyondur.\n",
    "\n",
    "![değer fonksiyonu](https://cdn-images-1.medium.com/max/1000/0*kvtRAhBZO-h77Iw1.)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**örnek değer durumu:**\n",
    "\n",
    "\n",
    "![örnek değer durumu](https://cdn-images-1.medium.com/max/1000/1*2_JRk-4O523bcOcSy1u31g.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "prZzjS-gAYsM"
   },
   "source": [
    "# Eylem Değer Fonksiyonu (Action Value Function)\n",
    "\n",
    "Bir durumda bir hareketin tercih edilmesi durumunda alınması beklenen ödül hesabı.\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1000/1*6IqzImIFK1oEiVWmlu1Esw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6lG1FpHYHmX6"
   },
   "source": [
    "\n",
    "# Q-learning:\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1250/1*jmcVWHHbzCxDc-irBy9JTw.png)\n",
    "\n",
    "Ya durum-eylem düzeyimiz çok büyük ise???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aw1U2QKaYld1"
   },
   "source": [
    "![alt text](https://imgv2-1-f.scribdassets.com/img/document/294823207/original/330c3b7089/1530579908?v=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EarNgkt2H34B"
   },
   "source": [
    "# Deep Learning in Action!\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1000/1*w5GuxedZ9ivRYqM_MLUxOQ.png)\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qap4-JgF3e0m"
   },
   "source": [
    "**Doom Oyunu**\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/800/1*Q4XjhLC0IAOznnk5613PsQ.gif)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aVsno2k2z9pF"
   },
   "source": [
    "**Vizdoom Kütüphanesinin yüklenmesi:**\n",
    "\n",
    "Linkten işletim sisteminize uygun vizdoom kütüphanesini yükleyebilirsiniz: https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md#installationbuilding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "81ob7Paz0soC"
   },
   "source": [
    "**Gerekli Kütüphanelerin Yüklenmesi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "23rdyaL-0ZFS"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf      \n",
    "import numpy as np           \n",
    "import vizdoom  as viz\n",
    "import os\n",
    "\n",
    "import random                \n",
    "import time                  \n",
    "from skimage import transform\n",
    "\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "configs_path = \"ayarlar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cVgZT3NxTUzH"
   },
   "outputs": [],
   "source": [
    "### MODEL ÜST DEĞİŞKENLERİ\n",
    "STATE_SIZE = [84,84,4]      \n",
    "ACTION_SIZE = 3              \n",
    "LEARNING_RATE =  0.0002   \n",
    "DISCOUNT_RATE = 0.95 \n",
    "\n",
    "### EĞİTİM ÜST DEĞİŞKENLERİ\n",
    "MAX_EPISODES = 500        \n",
    "max_steps = 300              \n",
    "BATCH_SIZE = 64             \n",
    "\n",
    "# EPSILON GREEDY \n",
    "explore_start = 1.0            \n",
    "explore_stop = 0.01            \n",
    "decay_rate = 0.0001             \n",
    "\n",
    "### HAFIZA ÜST DEĞİŞKENLERİ\n",
    "MEMORY_SIZE = 100000         \n",
    "\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wWU5s52M1uAU"
   },
   "source": [
    "**Çevrenin Oluşturulması**\n",
    "\n",
    "---\n",
    "\n",
    "Vizdoom'u kullanarak farklı senaryoları çevre olarak kullanabilir, farklı ayarlar ile çalışılınabilir. Bu yüzden Doom çevresi 2 tane ayar dosyası kullanır:\n",
    "\n",
    "\n",
    "1.   Çevrenin olası eylemleri ne olabilir, ekran büyüklüğü ne kadar olacak gibi ön tanımlı ayarlarını içeren bir *ayar* dosyası.\n",
    "2.   Çevrenin hangi senaryoyu uygulayacağını belirttiğimiz bir *senaryo* dosyası.\n",
    "\n",
    "Biz bu atölye de *basic* [senaryosunu](https://github.com/mwydmuch/ViZDoom/tree/master/scenarios) kullanacağız. Bu senaryoyu ve oyun çevremizin ayarlarını içeren *basic* dosyalarını indirip *ayarlar* klasörünün altına kayıt ediyoruz.\n",
    "\n",
    "Basic ayarlarından bazılarını aşağıdaki gibi güncelliyoruz:\n",
    "\n",
    "screen_format = GRAY8\n",
    "\n",
    "window_visible = false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4hLSSqeN1gp3"
   },
   "source": [
    "**Bizim Çevremizin Önemli Özellikleri**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*   Canavarlar karşı duvarda random olarak herhangi bir yerde canlanacaklar.\n",
    "*   Bir bölüm canavar öldüğünde ya da 300 sn geçtiğinde biter.\n",
    "*   Ajanımız canavarı öldürürse **+101 puan** alır.\n",
    "*   Her ıskaladığı atış için **-5 puan** alır.\n",
    "*   Eğer bölüm sonunda canavar ölmemişse **-1 puan** alır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ui2i5mpFPXQ9"
   },
   "outputs": [],
   "source": [
    "class GameEnv():\n",
    "  \n",
    "  def __init__(self, screen_format=viz.ScreenFormat.GRAY8, window_visible=False):\n",
    "    self.game = viz.DoomGame()\n",
    "    self.load_config(screen_format, window_visible)\n",
    "    self.game.init()\n",
    "    \n",
    "  def load_config(self, screen_format, window_visible,\n",
    "                  config_file='basic.cfg' , scenario_file = 'basic.wad'):\n",
    "    scenario_path = os.path.join(configs_path, scenario_file)\n",
    "    config_path = os.path.join(configs_path, config_file)\n",
    "    \n",
    "    self.game.load_config(config_path)\n",
    "    self.game.set_doom_scenario_path(scenario_path)\n",
    "    \n",
    "    self.game.set_screen_format(screen_format)\n",
    "    self.game.set_window_visible(window_visible)\n",
    "    \n",
    "    \n",
    "  def get_state(self):\n",
    "    return self.game.get_state().screen_buffer\n",
    "  \n",
    "    \n",
    "  def create_new_episode(self):\n",
    "    self.game.new_episode()\n",
    "    \n",
    "    return self.get_state()\n",
    "  \n",
    "  \n",
    "  def make_action(self, action):\n",
    "    reward = self.game.make_action(action)\n",
    "    done = self.game.is_episode_finished()\n",
    "    \n",
    "    if done:\n",
    "      next_state = np.zeros((84,84), dtype=np.int)\n",
    "    else:\n",
    "      next_state = self.get_state()\n",
    "      \n",
    "    return (reward, done, next_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hFOVfFxl6hj8"
   },
   "source": [
    "**Bizim Çevremizin 3 olası aksiyonu var:** \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "*   Sola git\n",
    "*   Sağa git\n",
    "*   Ateş et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6WdA-gmO6bcm"
   },
   "outputs": [],
   "source": [
    "LEFT = [1, 0, 0]\n",
    "RIGHT = [0, 1, 0]\n",
    "SHOOT = [0, 0, 1]\n",
    "\n",
    "possible_actions = [LEFT, RIGHT, SHOOT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t3LfHYcI7NwY"
   },
   "source": [
    "**Oyunun her bir frame'inin bir ön işlemden geçirilmesi:**\n",
    "\n",
    "---\n",
    "\n",
    "Bizim ajanımızın durum bilgisini oyunun frameleri oluşturmaktadır. Bizim çevremizde her bir frame 240x320 pixelden oluşmaktadır. Her bir adımda bu kadar 240x320 pixel işlemek bizim eğitimimizi yavaşlatıp, bilgisayarın kaynaklarını gereksiz sömürmesine neden olabilir. Bu sebepten framemimizi daha sade bir hale getirmek için bazı ön işlemlerden geçireceğiz.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1.   İlk adımımız resmin renklerini siyah-beyaz yapmak. Çünkü renk bilgisi bizim eğitimimiz için çok önemli bir bilgi değil. (Bu ön adım işlemini çevre ayarlarında screen_format'ı ScreenFormat.GRAY8 yaparak gerçekleştirdik.)\n",
    "2.   Resmin tavan kısmı yine eğitimimiz için gereksiz bir bilgidir. Bu tavan kısımlarını da kesmek anlamlı bir ön işlem olacaktır.\n",
    "3.   Son ön işlem adımı ise pixel bilgilerini normalize edip, 84x84 matrise çevirmedir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7fLjWRndA2Ea"
   },
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    # Crop the screen (remove the roof because it contains no information)\n",
    "    cropped_frame = frame[30:-10,30:-30]\n",
    "    \n",
    "    # Normalize Pixel Values\n",
    "    normalized_frame = cropped_frame/255.0\n",
    "    \n",
    "    # Resize\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [84,84])\n",
    "    \n",
    "    return preprocessed_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BfyCzC6S-QgP"
   },
   "source": [
    "**Frame Yığma İşlemi**\n",
    "\n",
    "---\n",
    "\n",
    "Sadece frame üzerinden eğitim yapmanın en büyük problemi hareketin ne yöne doğru gerçekleştiğini göremiyor olmak. Bu problemin önüne geçmek için eğitim modülümü frameleri üst üste yığarak gerçekleştireceğiz. Buradaki amaç 4 farklı frame'i kullanarak eylemin hangi yöne doğru gittiğini belirleyebilmek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BMfMBvenMsqd"
   },
   "outputs": [],
   "source": [
    "stack_size = 4 # Eğitim için 4 frame'i üst üste koyuyoruz.\n",
    "\n",
    "stacked_frames = deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4) # queue yapısını kullanıyoruz bu frame yığma işlemi için. Yeni bir frame geldiği zaman en eski frame'i queue'dan çıkarır.\n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        stacked_frames = deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        \n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "        \n",
    "    else:\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fn_zr2lWMD7g"
   },
   "source": [
    "**Derin Öğrenme Modelimiz:**\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1000/1*LglEewHrVsuEGpBun8_KTg.png)\n",
    "\n",
    "**Derin Öğrenme Eğitiminde kullandığımız Sıralama:**\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1000/1*RFt8MBBkUSPZdolp_WfZFA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XR5OwlDgCT_H"
   },
   "outputs": [],
   "source": [
    "# derin öğrenme modelimiz için yardımcı fonksiyonlar\n",
    "def create_variable(name, shape, init=None, std=None, wd=None):\n",
    "    if init is None:\n",
    "        if std is None:\n",
    "            std = (2./shape[0])**0.5\n",
    "            \n",
    "        init = tf.truncated_normal_initializer(stddev=std)\n",
    "        \n",
    "    with tf.device('/cpu:0'):\n",
    "        new_variables = tf.get_variable(name, shape, initializer=init, dtype=tf.float32)\n",
    "    \n",
    "    return new_variables\n",
    "  \n",
    "def dense_layer(name, inputs, units, activation = tf.nn.elu, \n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer()):\n",
    "  \n",
    "    return tf.layers.dense(inputs, units, \n",
    "                           activation = activation, \n",
    "                           kernel_initializer= kernel_initializer, \n",
    "                           name=name)\n",
    "  \n",
    "def conv_layer(name, inputs, filters, kernel_size=[4,4], strides = [2,2],\n",
    "         kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d()):\n",
    "  \n",
    "    return tf.layers.conv2d(inputs, filters, kernel_size, \n",
    "                            strides=strides, \n",
    "                            kernel_initializer = kernel_initializer, \n",
    "                            name=name)\n",
    "  \n",
    "def batch_norm_layer(name, conv_layer, training = True, epsilon = 1e-5):\n",
    "  \n",
    "    return tf.layers.batch_normalization(conv_layer, \n",
    "                                         training = training, \n",
    "                                         epsilon= epsilon, \n",
    "                                         name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZH_4jkEFDAC9"
   },
   "outputs": [],
   "source": [
    "# derin öğrenme modelimiz\n",
    "dqn = tf.Graph()\n",
    "\n",
    "with dqn.as_default():\n",
    "    with tf.name_scope('input'):\n",
    "      inputs_ = tf.placeholder(tf.float32, [None, *STATE_SIZE], name=\"inputs\")\n",
    "      actions_ = tf.placeholder(tf.float32, [None, ACTION_SIZE], name=\"actions_\")\n",
    "      target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "      \n",
    "    with tf.variable_scope('conv1') as scope:  # Giriş [84, 84, 4] --> Çıkış [20, 20, 32]\n",
    "        conv = conv_layer('conv1', inputs_, filters=32, kernel_size = [8,8], strides = [4,4])\n",
    "        batch_norm = batch_norm_layer('batch_norm1', conv)\n",
    "        out1 = tf.nn.elu(batch_norm, name=\"conv1_out\")\n",
    "        \n",
    "    with tf.variable_scope('conv2') as scope:  # Giriş [20, 20, 32] --> çıkış[9, 9, 64]\n",
    "        conv = conv_layer('conv2', out1, filters=64)\n",
    "        batch_norm = batch_norm_layer('batch_norm2', conv)\n",
    "        out2 = tf.nn.elu(batch_norm, name=\"conv2_out\")\n",
    "        \n",
    "    with tf.variable_scope('conv3') as scope:  # Giriş [9, 9, 64] --> çıkış [3, 3, 128]\n",
    "        conv = conv_layer('conv3', out2, filters=128)\n",
    "        batch_norm = batch_norm_layer('batch_norm3', conv)\n",
    "        out3 = tf.nn.elu(batch_norm, name=\"conv3_out\")\n",
    "        \n",
    "    with tf.variable_scope('fully_connected1') as scope: # Giriş [3, 3, 128] --> çıkış [1152]\n",
    "        flatten = tf.layers.flatten(out3) \n",
    "        fc1 = dense_layer('fc1', flatten, units = 512)\n",
    "        \n",
    "    with tf.variable_scope('output') as scope: # Giriş [1152] --> çıkış [3]\n",
    "        output_ = dense_layer('output', fc1, units= 3, activation=None)\n",
    "  \n",
    "    Q = tf.reduce_sum(tf.multiply(output_, actions_), axis=1)\n",
    "            \n",
    "            \n",
    "    # The loss is the difference between our predicted Q_values and the Q_target\n",
    "    # Sum(Qtarget - Q)^2\n",
    "    loss_ = tf.reduce_mean(tf.square(target_Q - Q))\n",
    "            \n",
    "    optimizer_ = tf.train.RMSPropOptimizer(LEARNING_RATE).minimize(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K-_1XNMp96z6"
   },
   "source": [
    "**Deneyim Tekrarlama (Experience Replay)**\n",
    "\n",
    "---\n",
    "\n",
    "Bir bölüm boyunca ajanımız farklı farklı durumlar gözlemliyor. Hatırlayacağınız gibi doom'un durum bilgisi anlık ekran(frame) bilgimizdi. Ekran pixelleri bizim durum bilgimiz olunca aynı durumu farklı bölümlerde tekrar denk gelme olasılığımız azalıyor. Fakat biz bu durum bilgisini unutmak istemiyoruz. Bizi başarıya götürecek yolda önemli bir durum olabilir.\n",
    "\n",
    "Bu sorunun önüne geçmek için ajanın deneyimlerini hafızada saklıyoruz. Eğitim sırasında rastgele olarak bu deneyimlerden seçip ajanı bu rastgele seçilmiş deneyimler ile eğiteceğiz. Bu yönteme ***deneyim tekrarlama (experience replay)*** denir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Phqxp9dNe3-"
   },
   "outputs": [],
   "source": [
    "class Experience():\n",
    "    def __init__(self, max_size = MEMORY_SIZE):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size = BATCH_SIZE):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size), size = batch_size, replace = False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]\n",
    "      \n",
    "    def batch_sample(self):\n",
    "        batch = self.sample()\n",
    "        \n",
    "        states = np.array([each[0] for each in batch], ndmin=3)\n",
    "        actions = np.array([each[1] for each in batch])\n",
    "        rewards = np.array([each[2] for each in batch]) \n",
    "        next_states = np.array([each[3] for each in batch], ndmin=3)\n",
    "        dones = np.array([each[4] for each in batch])\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "piAtjH1LYXcW"
   },
   "outputs": [],
   "source": [
    "experiences = Experience()\n",
    "env = GameEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pQbGnDLA0kr"
   },
   "source": [
    "**Epsilon Greedy Stratejisi**\n",
    "\n",
    "---\n",
    "\n",
    "Pekiştirmeli öğrenmenin en önemli problemlerinden biri *keşif ve sömürünün* dengede tutulabilmesidir. Eğer çok fazla keşif yaparsa bu sefer çok fazla rasgele eylem deneyecektir ve en iyi öneriyi bulması zorlaşacaktır. Diğer taraftan çok fazla sömürü yaparsa, bu sefer de ajan en iyi eylem olduğunu düşündüğü eylemi sürekli seçer fakat keşif yapmadığı için daha iyi bir eylem olup olmadığını bilemez. Dolayısıyla keşif ve sömürünün belli bir dengede olması gerekir.\n",
    "\n",
    "Epsilon greedy bu dengeyi gözeten yaklaşımlardan biridir. Belli bir epsilon olasılıkla rasgele bir eylem seçilir. Diğer durumlarda ajan kendisi için en yüksek getiriyi veren eylemi gerçekleştirir.\n",
    "\n",
    "Biz bu problemde bu epsilon greedy yaklaşımının bir gelişmişini kullandık. Eğitimin başlarında epsilon olasılığı 1 seçilir. Yani ajanımız rastgele aksiyon deneyecektir. Daha sonra zaman geçtikçe bu epsilon olasılığı *decay_rate* oranında düşecektir. Dolayısıyla eğitimin başlarında ajan daha çok *rastgele eylem* deneyerek çok fazla gözlem gerçekleştirir, daha sonra eğitim devam ettikçe bu epsilon oranı düşeceği için *en iyi(greedy) eylemi *seçmeye başlayacaktır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gbTD-rUUj5On"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    # Geliştirilmiş epsilon greedy stratejisi kullanılarak epsilon hesabı\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if (explore_probability > np.random.rand()): # Random değer eğer epsilon değerimizden küçük ise rastgele hareket seç = keşif(exploration)\n",
    "        action = random.choice(possible_actions)\n",
    "    else: # Random değer eğer epsilon değerimizden büyük ise greedy eylemi seç = sömürü(exploitation)\n",
    "\n",
    "        # Eylemlerin değerlerini hesap et\n",
    "        Qs = sess.run(output_, feed_dict = {\n",
    "                          inputs_: state.reshape((1, *state.shape))\n",
    "                      })\n",
    "        \n",
    "        # Maximum değere sahip eylemi seç (greedy eylem)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "                \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fsH43BhLFwC1"
   },
   "source": [
    "Eğitimimizi hafızadaki *batch_size* kadar deneyim ile gerçekleştireceğimizi deneyim tekrarlama başlığında söylemiştik. Öğrenimin başlangıcında eğitimi gerçekleştirebilmek için hafızamısı *batch_size* kadar rastgele eylem ile dolduruyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4vZYQYgUNr7W"
   },
   "outputs": [],
   "source": [
    "state = env.create_new_episode()\n",
    "state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "for i in range(BATCH_SIZE):\n",
    "    action = random.choice(possible_actions)\n",
    "    reward, done, next_state = env.make_action(action)\n",
    "    \n",
    "    if done:\n",
    "        experiences.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        state = env.create_new_episode()\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "    else:\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        \n",
    "        experiences.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_lA3FTLMIhPe"
   },
   "source": [
    "\n",
    "**Q-learning**\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1250/1*jmcVWHHbzCxDc-irBy9JTw.png)\n",
    "\n",
    "**Parametreleştirilmiş TD Hatası**\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1000/1*Zplt-1wTWu_7BGmZCBFjbQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fxmlZiU-Xx6K"
   },
   "outputs": [],
   "source": [
    "def learn_playing():\n",
    "  target_Qs_batch = []\n",
    "  states, actions, rewards, next_states, dones = experiences.batch_sample()\n",
    "\n",
    "  Qs_next_state = sess.run(output_, feed_dict = {inputs_: next_states})\n",
    "\n",
    "  # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + DISCOUNT_RATE*maxQ(s', a')\n",
    "  for i in range(0, BATCH_SIZE):\n",
    "      terminal = dones[i]\n",
    "\n",
    "      if terminal: # Eğer terminal durumunda ise Q_target = r\n",
    "          target_Qs_batch.append(rewards[i]) \n",
    "          \n",
    "      else: # Q_target = r + DISCOUNT_RATE*maxQ(s', a')\n",
    "          target = rewards[i] + DISCOUNT_RATE * np.max(Qs_next_state[i])\n",
    "          target_Qs_batch.append(target)\n",
    "\n",
    "  targets = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "  loss, _ = sess.run([loss_, optimizer_],\n",
    "                      feed_dict={inputs_: states,\n",
    "                                 target_Q: targets,\n",
    "                                 actions_: actions})\n",
    "  \n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AXKJUW0lNY3l"
   },
   "outputs": [],
   "source": [
    "def run_an_episode(episode, env, decay_step, stacked_frames):\n",
    "  total_reward = 0\n",
    "  step = 0\n",
    "  episode_rewards = []\n",
    "  \n",
    "  state = env.create_new_episode()\n",
    "  state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "  done = False\n",
    "  \n",
    "  while(not done):\n",
    "    step += 1\n",
    "    decay_step +=1\n",
    "\n",
    "    action, explore_probability = epsilon_greedy_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "\n",
    "    reward, done, next_state = env.make_action(action)\n",
    "\n",
    "    episode_rewards.append(reward)\n",
    "\n",
    "    if done:\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "        total_reward = np.sum(episode_rewards)\n",
    "\n",
    "        print('Episode: {}'.format(episode),\n",
    "                  'Total reward: {}'.format(total_reward),\n",
    "                  'Training loss: {:.4f}'.format(loss),\n",
    "                  'Explore P: {:.4f}'.format(explore_probability))\n",
    "\n",
    "        experiences.add((state, action, reward, next_state, done))\n",
    "    else:\n",
    "        if step % 10 == 0:\n",
    "          print('Episode: {}'.format(episode),\n",
    "                  'Step: {}'.format(step),\n",
    "                  'Action: {}'.format(action),\n",
    "                  'Training loss: {:.4f}'.format(loss),\n",
    "                  'Explore P: {:.4f}'.format(explore_probability))\n",
    "\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "        experiences.add((state, action, reward, next_state, done))\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    loss = learn_playing()\n",
    "  \n",
    "  return total_reward, decay_step, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1322,
     "status": "ok",
     "timestamp": 1539520499379,
     "user": {
      "displayName": "Furkan Arslan",
      "photoUrl": "",
      "userId": "03329637017513627976"
     },
     "user_tz": -180
    },
    "id": "uGWaZF4qlHFn",
    "outputId": "5f8811bc-a8ae-4e99-c94f-151620019d9a"
   },
   "outputs": [],
   "source": [
    "with dqn.as_default():\n",
    "  saver = tf.train.Saver()\n",
    "  sess = tf.Session()\n",
    "\n",
    "  try:\n",
    "      checkpoint_dir = tf.train.latest_checkpoint(checkpoint_dir=\"./models/\")\n",
    "      saver.restore(sess, save_path=checkpoint_dir)\n",
    "      print(\"Restored checkpoint from:\", checkpoint_dir)\n",
    "  except:\n",
    "      print(\"Initializing variables\")\n",
    "      sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 152133
    },
    "colab_type": "code",
    "id": "UY7FWyVxNO99",
    "outputId": "a75978fa-0078-4c28-f413-268d21bdfef8"
   },
   "outputs": [],
   "source": [
    "decay_step = 0\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    total_reward, decay_step, stacked_frames = run_an_episode(episode, env, decay_step, stacked_frames)\n",
    "    \n",
    "    # Save model every 5 episodes\n",
    "    if episode % 5 == 0:\n",
    "        save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "        print(\"\\n Episode: %d, Model Saved\" % (episode))\n",
    "\n",
    "    print('*'*70)\n",
    "    print('Episode Finished! Total Reward is %d\\n' % (total_reward))\n",
    "    print('*'*70) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "spMy-yiP-Fc2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
